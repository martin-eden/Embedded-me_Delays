( Delays

  Typical source code describes what has to be done. It does not
  describe how long it should take.

  Any communication takes time. No matter how fast you can flip bit
  on output pin, for communication you need precision, not speed.

) 2025-08-21

( Sleeps

  "Sleep" is human term for inactivity of about 1/3 time of work cycle.

  It does not have much sense for microcontrollers but useful
  when interfacing with human.

  Human can estimate time say between 0.2 s and 1.6 s without losing
  precision.

  Point of "sleep" is increase endurance by reducing power.
  Not for precision time delay.

  "Sleep" is always goes with "wake up conditions".

) 2025-08-21

( Time

  You cannot measure time without external input.

  Historically "day" is time span between "noons".

  And "noon" is time moment when you observe shortest shadow from pole.

  Without input we are counters. We will count our ticks.
  But we don't know how often those tick really occur.
  We hope they occur 16 mils per second (F_CPU value).

) 2025-08-21

( Precision

  Microseconds-calling function compensates for cycle overhead and
  setup cost.

  Still for 20-microseconds test I've ran with oscilloscope
  time spans are ~ 22.4 us.

  Test is

    while(1)
    {
      Led.Write(1);
      me_Delays::Delay_Us(Led_TimeOn_Us);
      Led.Write(0);
      me_Delays::Delay_Us(Led_TimeOff_Us);
    }

  I guess that 2.4 micros is time to return from Delay_Us() and
  reach pin writing instruction in Led.Write(0). We can't compensate it.

  (So no, this module is not suitable for implementing software UART
  working at 1 megabit speed. That's why we have hardware UART -
  precision timing.)

  But I've not met device that accepts pulse say only between 48 and 49
  micros.


  Theoretical precision is 62.5 ns (for 16 MHz).

  You need to use assembly for that time granularity.
  (It was required for RGB LED stripe on WS2812b chipset.)

) 2025-08-21

(
  Delays are not trivial

  Okay, we have DelayMicrosecond() function that does exactly
  one microsecond delay per call.

  ( Well, only for hardcoded 16 MHz frequency. )

  You can do two microseconds delay by two hardcoded calls.

  You write "for" cycle for delays from 1 to 100. And you'll discover
  that execution takes longer time. Because of cycle overhead.
  You can compensate it tho.

  Can you do longer delays?

) 2025-10-24

(
  Longer delays problem

  We have run-time clock. It's interrupt handler too.
  On hardware counter event. It advances tracked run time.
  And that code takes some time.

  We have it because event handlers are interrupt handlers and
  they need run time to register their event.

  If you implement DelaySecond() as million calls of DelayMicrosecond()
  actual time will be longer. Because your code will be diluted by
  code of run-time interrupt handler.

  If you disable interrupts for DelaySecond(), your delay will become
  precise. But you fucked up system clock. And registered events
  will get wrong time.

  Wrong time defeats purpose of run-time tracking. You can no longer
  rely that you can replay events.

) 2025-10-24

(
  Longer delays solution

  Say you have date with girl in Constantinople at 53 days 16 hours
  7 minutes and 53 seconds from this time.

  You don't need to check time every second of this period.

  You can calculate date's time now and then spend ordinary 53 days.
  Then you start tracking hours, minutes and seconds.

  We're doing similar thing. For longer delays we're spending time
  just watching at run-time clock. No interrupts disabled. Then we
  disable interrupts and do final stride.

  (
    Run-time clock event here and now ticks every millisecond.
    Disabling interrupts does not stop it's counter. Time update
    event will be registered when it occurs. And will be processed
    when you enable interrupts back.

    But you can disable interrupts only for less than one millisecond.
    Or else two time update events may occur and only one of them
    will be processed. Because it's bit flag.
  )

  But she'll be waiting you in Istanbul.

) 2025-10-24

(
  There is no sense in long delays without interrupts.
  There is no sense in short delays with interrupts.

  So there is no sense in delay function doing both long and
  precise delay.

) 2025-10-27
